{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HIgl6RXOptw"
   },
   "source": [
    "# CIFAR10 Classfier: Optuna Edition\n",
    "\n",
    "Our objective is similar to the keras-tuner and ray tune notebooks:\n",
    "- explore optuna for hyperparam tuning\n",
    "- find out if we can beat the test accuracy of **0.0** see: `<notebook>`\n",
    "\n",
    "Author: Katnoria | Created: 18-Oct-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports & Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HInAIRAyTb9E"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import Model\n",
    "import IPython\n",
    "import optuna\n",
    "from optuna.integration.tensorboard import TensorBoardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aFeWfMrDOpt0"
   },
   "outputs": [],
   "source": [
    "def version_info(cls):\n",
    "    print(f\"{cls.__name__}: {cls.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "cA97cWBpOpt2",
    "outputId": "89c127f2-b262-4941-eae8-04dd7fae527c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Used in this Notebook:\n",
      "tensorflow: 2.3.0\n",
      "tensorflow_datasets: 3.2.1\n",
      "optuna: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Version Used in this Notebook:\")\n",
    "version_info(tf)\n",
    "version_info(tfds)\n",
    "version_info(optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QQCUah4vOpt4",
    "outputId": "59895942-e70e-4cae-9c1a-c915f105d6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "# EPOCHS = 2\n",
    "BATCH_SIZE=128\n",
    "IMG_SIZE=32\n",
    "NUM_CLASSES=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyDWhutEOpt8"
   },
   "source": [
    "# 2. Dataset\n",
    "\n",
    "Tensorflow Datasets already provides this dataset in a format that we can use out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/optuna/optuna/blob/master/examples/tensorflow_eager_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    (ds_train, ds_test), metadata = tfds.load(\n",
    "        'cifar10', split=['train', 'test'], shuffle_files=True, \n",
    "        with_info=True, as_supervised=True\n",
    "    )\n",
    "    \n",
    "    train_ds = ds_train \\\n",
    "        .cache() \\\n",
    "        .batch(BATCH_SIZE, drop_remainder=True) \\\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    test_ds = ds_test \\\n",
    "        .cache() \\\n",
    "        .batch(BATCH_SIZE, drop_remainder=True) \\\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return (train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model\n",
    "\n",
    "We will use the same transforms that were using in training hand tuned TensorFlow notebooks.\n",
    "\n",
    "## 3.1 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Model\n",
    "def create_model(trial):\n",
    "    \"\"\"\n",
    "    Create a simple CIFAR-10 model that uses ResNet50 as its backbone.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    trial: optuna Trial object\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = transforms(inputs)\n",
    "    x = tf.keras.applications.resnet.preprocess_input(x)\n",
    "    x = tf.keras.applications.ResNet50(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False)(x, training=False)\n",
    "    # Flatten or GAP\n",
    "    use_gap = trial.suggest_categorical('use_gap', [True, False])\n",
    "    if use_gap:\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    else:\n",
    "        x = Flatten()(x)\n",
    "    x = Flatten()(x)\n",
    "    # Dense Layer Units\n",
    "    num_hidden = trial.suggest_int('dense_1', 32, 128)\n",
    "    # Activation\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'selu', 'elu'])\n",
    "    x = Dense(128, activation=activation)(x)\n",
    "    # Dropout rate    \n",
    "    drop_rate = trial.suggest_float('drop_rate', 0.0, 0.8)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "    outputs = Dense(NUM_CLASSES)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Optimizers\n",
    "\n",
    "We could add various optimizers to the search space. I'll leave that for you to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Optimizer\n",
    "def create_optimizer(trial):\n",
    "    # LR\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Objective\n",
    "def train(model, optimizer, dataset, mode=\"eval\"):\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)    \n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "    mean_loss = tf.keras.metrics.Mean(name=\"loss\")\n",
    "    for images, labels in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images, training=(mode=='train'))\n",
    "            loss = loss_object(labels, predictions)            \n",
    "            if mode == \"train\":\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            accuracy(labels, predictions)\n",
    "            mean_loss(loss)\n",
    "    return accuracy.result(), mean_loss.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Trials\n",
    "\n",
    "## 4.1 Setup Objective\n",
    "\n",
    "We define the objective function that Optuna should optimize. In our case, its the test accuracy over a certain number of `EPOCHS`.\n",
    "You can improve the search efficient by letting Optuna prune the unpromising trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PthJrYv2TpVy"
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # dataset\n",
    "    train_ds, test_ds = get_dataset()\n",
    "    # model\n",
    "    model = create_model(trial)\n",
    "    # optimizer\n",
    "    optimizer = create_optimizer(trial)\n",
    "    # train\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_acc, train_loss = train(model, optimizer, train_ds, \"train\")\n",
    "        test_acc, test_loss = train(model, optimizer, test_ds, \"eval\")\n",
    "        trial.report(test_acc, epoch)\n",
    "        print(f\"train_accuracy:{train_acc:.4f}, train_loss: {train_loss:.4f}, test_acc: {test_acc:.4f}\")\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Run Trials\n",
    "\n",
    "We are now ready to run optuna and find the best set of hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/miniconda3/envs/tf2_3/lib/python3.6/site-packages/ipykernel_launcher.py:2: ExperimentalWarning: TensorBoardCallback is experimental (supported from v2.0.0). The interface can change in the future.\n",
      "  \n",
      "\u001b[32m[I 2020-10-21 11:22:21,476]\u001b[0m A new study created in memory with name: no-name-0fb6a9b3-81e4-4b13-a494-974ec9cea169\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy:0.5205, train_loss: 1.3534, test_acc: 0.6755\n",
      "train_accuracy:0.6663, train_loss: 0.9624, test_acc: 0.7300\n",
      "train_accuracy:0.7090, train_loss: 0.8409, test_acc: 0.7423\n",
      "train_accuracy:0.7325, train_loss: 0.7766, test_acc: 0.7612\n",
      "train_accuracy:0.7474, train_loss: 0.7279, test_acc: 0.7724\n",
      "train_accuracy:0.7648, train_loss: 0.6791, test_acc: 0.7628\n",
      "train_accuracy:0.7727, train_loss: 0.6483, test_acc: 0.7711\n",
      "train_accuracy:0.7870, train_loss: 0.6148, test_acc: 0.7734\n",
      "train_accuracy:0.7959, train_loss: 0.5919, test_acc: 0.7776\n",
      "train_accuracy:0.8052, train_loss: 0.5624, test_acc: 0.7837\n",
      "train_accuracy:0.8098, train_loss: 0.5452, test_acc: 0.7851\n",
      "train_accuracy:0.8191, train_loss: 0.5196, test_acc: 0.7915\n",
      "train_accuracy:0.8255, train_loss: 0.4976, test_acc: 0.7678\n",
      "train_accuracy:0.8311, train_loss: 0.4801, test_acc: 0.7833\n",
      "train_accuracy:0.8385, train_loss: 0.4645, test_acc: 0.7758\n",
      "train_accuracy:0.8429, train_loss: 0.4510, test_acc: 0.7774\n",
      "train_accuracy:0.8511, train_loss: 0.4288, test_acc: 0.7824\n",
      "train_accuracy:0.8561, train_loss: 0.4120, test_acc: 0.7764\n",
      "train_accuracy:0.8599, train_loss: 0.3994, test_acc: 0.7807\n",
      "train_accuracy:0.8658, train_loss: 0.3825, test_acc: 0.7890\n",
      "train_accuracy:0.8727, train_loss: 0.3672, test_acc: 0.7817\n",
      "train_accuracy:0.8773, train_loss: 0.3537, test_acc: 0.7930\n",
      "train_accuracy:0.8793, train_loss: 0.3460, test_acc: 0.7887\n",
      "train_accuracy:0.8819, train_loss: 0.3397, test_acc: 0.7755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-10-21 11:38:29,271]\u001b[0m Trial 0 finished with value: 0.7918669581413269 and parameters: {'use_gap': False, 'dense_1': 69, 'activation': 'relu', 'drop_rate': 0.07607458016208418, 'learning_rate': 8.362974203545105e-05}. Best is trial 0 with value: 0.7918669581413269.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy:0.8877, train_loss: 0.3200, test_acc: 0.7919\n"
     ]
    }
   ],
   "source": [
    "# Track using Tensorboard\n",
    "tensorboard_cb = TensorBoardCallback(\"./logs/\", metric_name=\"accuracy\")\n",
    "\n",
    "start = time()\n",
    "# Run\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1, timeout=600, callbacks=[tensorboard_cb])\n",
    "stop = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "took = stop - start\n",
    "print(f\"Total time: {took//60 : .0f}m {took%60:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Inspect\n",
    "\n",
    "Print out the information about the trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Trials: 1\n",
      "Pruned Trials: 0\n",
      "Completed Trials: 1\n"
     ]
    }
   ],
   "source": [
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(f\"Finished Trials: {len(study.trials)}\")\n",
    "print(f\"Pruned Trials: {len(pruned_trials)}\")\n",
    "print(f\"Completed Trials: {len(complete_trials)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7918669581413269\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print(trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gap: False\n",
      "dense_1: 69\n",
      "activation: relu\n",
      "drop_rate: 0.07607458016208418\n",
      "learning_rate: 8.362974203545105e-05\n"
     ]
    }
   ],
   "source": [
    "for k,v in trial.params.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CIFAR10-gpu-tf2-keras tuner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf2_3]",
   "language": "python",
   "name": "conda-env-tf2_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
