{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classfication Network Architecture\n",
    "\n",
    "I am unable to get flowers classification network to get a validation accuracy > 48%. Try out different techniques including the same architecture I used to train CIFAR10 images\n",
    "\n",
    "@date: 06-Aug-2020 | @author: katnoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_info(cls):\n",
    "    print(f\"{cls.__name__}: {cls.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Used in this Notebook:\n",
      "tensorflow: 2.3.0\n",
      "tensorflow_datasets: 3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Version Used in this Notebook:\")\n",
    "version_info(tf)\n",
    "version_info(tfds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Tensorflow Datasets already provides this dataset in a format that we can use out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(ds_train, ds_test), metadata = tfds.load(\n",
    "    'cifar10', split=['train', 'test'], shuffle_files=True, \n",
    "    with_info=True, as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train), len(ds_test), metadata.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the built in function to visualise the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "show_examples() missing 1 required positional argument: 'ds_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-492c5f5407e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the built-in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: show_examples() missing 1 required positional argument: 'ds_info'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'id': Text(shape=(), dtype=tf.string),\n",
       "    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review metadata\n",
    "# See https://www.tensorflow.org/datasets/overview\n",
    "metadata.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 50000\n",
      "Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = metadata.features[\"label\"].num_classes\n",
    "\n",
    "num_train_examples = len(ds_train)\n",
    "num_test_examples = len(ds_test)\n",
    "print(f\"Training dataset size: {num_train_examples}\")\n",
    "print(f\"Test dataset size: {num_test_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "#     image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image / 255., label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds_train.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .cache() \\\n",
    "    .shuffle(num_train_examples).batch(BATCH_SIZE, drop_remainder=True) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((32, 32, 3), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ds_test.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE, drop_remainder=True) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "\n",
    "We now build a simple convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowersModel(Model):\n",
    "    def __init__(self):\n",
    "        super(FlowersModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(64, 3, padding='same')\n",
    "        self.pool1 = MaxPool2D(3, 2)\n",
    "        self.bn2 = BatchNormalization()        \n",
    "        self.pool2 = MaxPool2D(3, 2)        \n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        self.dense2 = Dense(NUM_CLASSES)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "#         x = self.bn1(x, training=training)\n",
    "        x = self.pool1(tf.nn.relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "#         x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool2(x)\n",
    "#         x = self.gap(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=0.2)\n",
    "        out = self.dense2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowersModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to measure the train and test accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "\n",
    "# Test\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "    # collect the gradients and apply\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # loss & acc\n",
    "    train_loss(loss)\n",
    "    train_acc(labels, predictions)\n",
    "    \n",
    "    \n",
    "# Test step    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    loss = loss_fn(labels, predictions)\n",
    "    # loss & acc\n",
    "    test_loss(loss)\n",
    "    test_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "Now, its time to train the model for N epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=1.4669, accuracy: 0.4736 :: test loss=1.4060, test accuracy: 0.4976\n",
      "Epoch 11: loss=1.3795, accuracy: 0.5052 :: test loss=1.3156, test accuracy: 0.5318\n",
      "Epoch 21: loss=1.3261, accuracy: 0.5241 :: test loss=1.2891, test accuracy: 0.5435\n",
      "Epoch 31: loss=1.2658, accuracy: 0.5456 :: test loss=1.2228, test accuracy: 0.5704\n",
      "Epoch 41: loss=1.2273, accuracy: 0.5628 :: test loss=1.1942, test accuracy: 0.5824\n",
      "Epoch 51: loss=1.1812, accuracy: 0.5797 :: test loss=1.1599, test accuracy: 0.5907\n",
      "Epoch 61: loss=1.1516, accuracy: 0.5892 :: test loss=1.1262, test accuracy: 0.6005\n",
      "Epoch 71: loss=1.1177, accuracy: 0.6030 :: test loss=1.1111, test accuracy: 0.6060\n",
      "Epoch 81: loss=1.0945, accuracy: 0.6098 :: test loss=1.0980, test accuracy: 0.6127\n",
      "Epoch 91: loss=1.0611, accuracy: 0.6198 :: test loss=1.0816, test accuracy: 0.6169\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "print_every = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    for tst_images, tst_labels in test_ds:\n",
    "        test_step(tst_images, tst_labels)\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss={train_loss.result():.4f}, accuracy: {train_acc.result():.4f} :: test loss={test_loss.result():.4f}, test accuracy: {test_acc.result():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Keras fit Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/195 [..............................] - ETA: 0s - loss: 2.4158 - accuracy: 0.0820WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 2s 10ms/step - loss: 1.4477 - accuracy: 0.4803 - val_loss: 2.3369 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 1.0864 - accuracy: 0.6147 - val_loss: 6.5635 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.9503 - accuracy: 0.6647 - val_loss: 9.6181 - val_accuracy: 0.1012\n",
      "Epoch 4/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.8772 - accuracy: 0.6907 - val_loss: 34.9941 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.8220 - accuracy: 0.7115 - val_loss: 66.8259 - val_accuracy: 0.1033\n",
      "Epoch 6/10\n",
      "195/195 [==============================] - 2s 10ms/step - loss: 0.7678 - accuracy: 0.7300 - val_loss: 61.1285 - val_accuracy: 0.1003\n",
      "Epoch 7/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.7311 - accuracy: 0.7434 - val_loss: 137.5869 - val_accuracy: 0.0999\n",
      "Epoch 8/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.6918 - accuracy: 0.7573 - val_loss: 58.9506 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.6615 - accuracy: 0.7678 - val_loss: 26.2375 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.6349 - accuracy: 0.7770 - val_loss: 24.8745 - val_accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f697f40a240>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FlowersModel()\n",
    "model.compile(\n",
    "    loss=loss_fn, \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],    \n",
    ")\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2_3]",
   "language": "python",
   "name": "conda-env-tf2_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
