{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classfication Network Architecture\n",
    "\n",
    "I am unable to get flowers classification network to get a validation accuracy > 48%. Try out different techniques including the same architecture I used to train CIFAR10 images\n",
    "\n",
    "@date: 06-Aug-2020 | @author: katnoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_info(cls):\n",
    "    print(f\"{cls.__name__}: {cls.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Used in this Notebook:\n",
      "tensorflow: 2.3.0\n",
      "tensorflow_datasets: 3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Version Used in this Notebook:\")\n",
    "version_info(tf)\n",
    "version_info(tfds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Tensorflow Datasets already provides this dataset in a format that we can use out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(ds_train, ds_test), metadata = tfds.load(\n",
    "    'cifar10', split=['train', 'test'], shuffle_files=True, \n",
    "    with_info=True, as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train), len(ds_test), metadata.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the built in function to visualise the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'id': Text(shape=(), dtype=tf.string),\n",
       "    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review metadata\n",
    "# See https://www.tensorflow.org/datasets/overview\n",
    "metadata.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 50000\n",
      "Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = metadata.features[\"label\"].num_classes\n",
    "\n",
    "num_train_examples = len(ds_train)\n",
    "num_test_examples = len(ds_test)\n",
    "print(f\"Training dataset size: {num_train_examples}\")\n",
    "print(f\"Test dataset size: {num_test_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "#     image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image / 255., label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds_train.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .cache() \\\n",
    "    .shuffle(num_train_examples).batch(BATCH_SIZE, drop_remainder=True) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((32, 32, 3), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ds_test.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE, drop_remainder=True) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "\n",
    "We now build a simple convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowersModel(Model):\n",
    "    def __init__(self):\n",
    "        super(FlowersModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, padding='same')\n",
    "#         self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(64, 3, padding='same')\n",
    "        self.pool1 = MaxPool2D(3, 2)\n",
    "#         self.bn2 = BatchNormalization()        \n",
    "        self.pool2 = MaxPool2D(3, 2)        \n",
    "        self.conv3 = Conv2D(128, 3, padding='same')\n",
    "        self.pool3 = MaxPool2D(3, 2)            \n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        self.dense2 = Dense(NUM_CLASSES)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        # using batchnorm results in very low test accuracies\n",
    "        # https://stackoverflow.com/questions/40081697/getting-low-test-accuracy-using-tensorflow-batch-norm-function\n",
    "        # Probably this could help\n",
    "#         x = self.bn1(x, training=training)\n",
    "        x = self.pool1(tf.nn.leaky_relu(x))\n",
    "        # conv 2\n",
    "        x = self.conv2(x)\n",
    "#         x = self.bn2(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.pool2(x)\n",
    "        # conv 3\n",
    "        x = self.conv3(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.pool3(x)\n",
    "        # GAP + Linear\n",
    "        x = self.gap(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=0.2)\n",
    "        out = self.dense2(x)\n",
    "        return out\n",
    "    \n",
    "class FlowersModelBN(Model):\n",
    "    def __init__(self):\n",
    "        super(FlowersModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(64, 3, padding='same')\n",
    "        self.pool1 = MaxPool2D(3, 2)\n",
    "        self.bn2 = BatchNormalization()        \n",
    "        self.pool2 = MaxPool2D(3, 2)        \n",
    "        self.flatten = Flatten()\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        self.dense1 = Dense(128, activation='relu')        \n",
    "        self.dense2 = Dense(NUM_CLASSES)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        # using batchnorm results in very low test accuracies\n",
    "        # https://stackoverflow.com/questions/40081697/getting-low-test-accuracy-using-tensorflow-batch-norm-function\n",
    "        # Probably this could help\n",
    "#         x = self.bn1(x, training=training)\n",
    "        x = self.pool1(tf.nn.leaky_relu(x))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "#         x = self.bn2(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=0.2)\n",
    "        out = self.dense2(x)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowersModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to measure the train and test accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "\n",
    "# Test\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "    # collect the gradients and apply\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # loss & acc\n",
    "    train_loss(loss)\n",
    "    train_acc(labels, predictions)\n",
    "    \n",
    "    \n",
    "# Test step    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    loss = loss_fn(labels, predictions)\n",
    "    # loss & acc\n",
    "    test_loss(loss)\n",
    "    test_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "Now, its time to train the model for N epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101: loss=0.9519, accuracy: 0.6601 :: test loss=0.9616, test accuracy: 0.6582\n",
      "Epoch 201: loss=0.8008, accuracy: 0.7150 :: test loss=0.8934, test accuracy: 0.6847\n",
      "Epoch 301: loss=0.7276, accuracy: 0.7389 :: test loss=0.8986, test accuracy: 0.6996\n",
      "Epoch 401: loss=0.6607, accuracy: 0.7612 :: test loss=0.8968, test accuracy: 0.7161\n",
      "Epoch 501: loss=0.6333, accuracy: 0.7736 :: test loss=0.9269, test accuracy: 0.7085\n",
      "Epoch 1001: loss=0.5242, accuracy: 0.8095 :: test loss=1.0011, test accuracy: 0.7085\n"
     ]
    }
   ],
   "source": [
    "# With 2 conv layers + GAP + 2 Linear Layers\n",
    "num_epochs = 1000\n",
    "print_every = int(0.1 * num_epochs)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    for tst_images, tst_labels in test_ds:\n",
    "        test_step(tst_images, tst_labels)\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss={train_loss.result():.4f}, accuracy: {train_acc.result():.4f} :: test loss={test_loss.result():.4f}, test accuracy: {test_acc.result():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101: loss=2.3031, accuracy: 0.0977 :: test loss=2.3030, test accuracy: 0.1000\n",
      "Epoch 201: loss=2.3032, accuracy: 0.0989 :: test loss=2.3027, test accuracy: 0.1002\n",
      "Epoch 301: loss=2.3033, accuracy: 0.0986 :: test loss=2.3028, test accuracy: 0.1001\n"
     ]
    }
   ],
   "source": [
    "# With 3 conv layers + GAP + 2 Linear Layers\n",
    "num_epochs = 1000\n",
    "print_every = int(0.1 * num_epochs)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    for tst_images, tst_labels in test_ds:\n",
    "        test_step(tst_images, tst_labels)\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss={train_loss.result():.4f}, accuracy: {train_acc.result():.4f} :: test loss={test_loss.result():.4f}, test accuracy: {test_acc.result():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.save_model.save(model, \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Keras fit Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/195 [..............................] - ETA: 0s - loss: 2.4158 - accuracy: 0.0820WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0052s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 2s 10ms/step - loss: 1.4477 - accuracy: 0.4803 - val_loss: 2.3369 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 1.0864 - accuracy: 0.6147 - val_loss: 6.5635 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.9503 - accuracy: 0.6647 - val_loss: 9.6181 - val_accuracy: 0.1012\n",
      "Epoch 4/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.8772 - accuracy: 0.6907 - val_loss: 34.9941 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.8220 - accuracy: 0.7115 - val_loss: 66.8259 - val_accuracy: 0.1033\n",
      "Epoch 6/10\n",
      "195/195 [==============================] - 2s 10ms/step - loss: 0.7678 - accuracy: 0.7300 - val_loss: 61.1285 - val_accuracy: 0.1003\n",
      "Epoch 7/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.7311 - accuracy: 0.7434 - val_loss: 137.5869 - val_accuracy: 0.0999\n",
      "Epoch 8/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.6918 - accuracy: 0.7573 - val_loss: 58.9506 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.6615 - accuracy: 0.7678 - val_loss: 26.2375 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "195/195 [==============================] - 2s 9ms/step - loss: 0.6349 - accuracy: 0.7770 - val_loss: 24.8745 - val_accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f697f40a240>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FlowersModel()\n",
    "model.compile(\n",
    "    loss=loss_fn, \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],    \n",
    ")\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2_3]",
   "language": "python",
   "name": "conda-env-tf2_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
